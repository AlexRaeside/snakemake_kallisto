
# read in yaml


configfile: "src/test.yaml"


##--- required modules ---------------------------------------------------------

import snakemake_modules.kallisto as kal
import snakemake_modules.metadata as meta
import pandas as pd


##--- global vars --------------------------------------------------------------

fastqc_dir = config["output_dir"] + "/fastqc"
sample_dir = config["output_dir"] + "/sample_table" 
quant_dir = config["output_dir"] + "/quant"
tables_dir = config["output_dir"] + "/tables"
metadata_dir = config["output_dir"] + "/metadata"

##---- snakemake functions -----------------------------------------------------
# Different then the functions in the modules as snakemake functions
# take one argument (the wildcard) and use global vars like the config data


def fastq_path(sample_name):
    
    """
    Using the sample name, return the file path to the corresponding 
    fastq file in the sample_table/sample_table.csv. Uses the sample_dir global
    variable to find the sample table.
    
    
    """
    
    
    # load in sample table 
    sample = pd.read_csv(sample_dir + "/sample_table.csv") 
    sample_info = sample[sample["sample_name"] == str(sample_name)]
    fastq = sample_info["fastqR1_file"].loc[sample_info.index[0]]
    return(fastq)


def get_mean_len(sample_name):
    
    """
    Using the sample name, return the mean read length for the sample. The 
    mean read lenght is located in the sample fastqc report which must be produced
    prior to running this function. The function uses the global variable 
    fastqc_dir to locate the sample fastqc reports. Returns the read length 
    number as a string.
    
    """
    
    
    fastqc_report = fastqc_dir + "/" + str(sample_name) + "_fastqc/fastqc_data.txt"
    
    file = open(fastqc_report)
  
    # read the content of the file opened
    length_line =  file.readlines()[8]
    length = length_line.split()[2]
    return(length)


##--- generate sample table ----------------------------------------------------

# Since the sample names in the sample_table is a key input parameter for many
# rules. The sample table is generated as soon as the snakemake is executed 
# unless there is already a sample table and the config 
# does not state to make a new one each time the snakemake is executed


if not os.path.exists(
    sample_dir + "/sample_table.csv") or config["make_sample_table"] == True:
    
    """
    
    By setting config["make_sample_table"] to false the user can add their #
    own sample_table.csv in the sample_table folder. If the user does not state 
    to use a prexisting the sample_table.csv then a new sample_table.csv is 
    generated either using the list of the fastq file paths in 
    config["fastq_R1"] or by selecting all fastq files in the directory in
    config["fastq_dir"]
    
    """
    
    
    # if the fastq files are give in config use in meta
    if len(config["fastq_R1"]) != 0:
        fq_paths = config["fastq_R1"]
    # if the fastq files are not given in config get from dir 
    if len(config["fastq_R1"]) == 0:
        fq_paths =  meta.get_fastq_in_dir(config["fastq_dir"])
    # use fastq files to generate a metadata table 
    meta.write_sample_table(fq_paths, config["output_dir"])



# From the sample table read the list of samples of process within the 
# snakemake. The snakemake will act on process different samples in 
# parallel though the first sample will be processed with kallisto_quant
# first in order the get the list and order of targets prodced  
# to make blank tables where results from all samples can be aggregated

SAMPLES = meta.get_samples(sample_dir + "/sample_table.csv")
first_sample = SAMPLES[0]



##--- Snakemake rules ----------------------------------------------------------



rule create_metadata:
    """
    Copy the metadata file in config[metadata] to the
    metadata folder as metadata/metadata.csv. If no
    metadata is provided then create one with just one
    column for the sample names. The sample names must
    always be the first column in the metadata table.
    
    """
    input: 
        metadata = config["metadata"]
    output: 
        metadata = metadata_dir + "/metadata.csv"
    
    shell: 
        "cp {input.metadata} {output.metadata}"





rule kallisto_index:
    """
    Takes the path to [transcripts].fa and creates a kallisto index 
    called [transcripts].idx which is used in pseduoalignments. Will 
    be run once per snakemake pipeline.
    
    """
    input:
        fasta = config["fasta"]
    output:
        index = config["output_dir"] + "/index/" + kal.get_indx_name(config["fasta"])
    shell:
        "kallisto index --index={output.index} {input.fasta}"
    
    
rule fastqc:
    """
    
    When running kallisto on single-end reads, an estimate of the average 
    read length is needed for kallisto quant. Mean read length is produced along
    with other QC metrics by FastQC. 
    
    This rule uses the 'sample' wildcard which will correspond to any of 
    the different samples in the SAMPLES variable.
    
    
    """
    input: 
        fq = fastq_path,
        sample_tbl = sample_dir + "/sample_table.csv"
    output:
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    run:
        if not os.path.exists(fastqc_dir):
            os.mkdir(fastqc_dir)
        shell("fastqc -o {fastqc_dir} --extract {input.fq}")



rule kallisto_quant:
    """
    
    Run kallisto quant on samples, using the corresponding fastq file 
    from the sample_table.csv, the kallsito index and mean read length
    found in the fastQC report.
    
    
    This rule uses the 'sample' wildcard which will correspond to any of 
    the different samples in the SAMPLES variable.
    
    
    """
    input:
        fq = fastq_path,
        index = config["output_dir"] + "/index/" + kal.get_indx_name(config["fasta"]),
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    output: 
        count_tbl = quant_dir + "/{sample}/abundance.tsv",
        count_summary = quant_dir + "/{sample}/run_info.json",
        quant_done = config["output_dir"] + "/done/quant_{sample}.txt"
    params: 
        thread = config["threads"],
        mean_len = get_mean_len,
        out_path = quant_dir + "/{sample}"
    run:
        if not os.path.exists(quant_dir):
            os.mkdir(quant_dir)
        # get average read length from fastqc report 
        shell(
            "kallisto quant -i {input.index} -o {params.out_path}"
            " --single -l {params.mean_len} -s 20  --single-overhang "
            "-t {params.thread} {input.fq}")
        shell(
            "touch {output.quant_done}"
        )


rule create_blank_files:
    input:
        count_tbl = quant_dir + "/" + first_sample + "/abundance.tsv"
    output:
        total_counts_tbl = tables_dir +"/counts_raw.csv",
        total_counts_summary = tables_dir +"/counts_summary.csv"
    run:
        
        if not os.path.exists(tables_dir):
            os.mkdir(tables_dir)
        
        kal.create_blank_tbls(input.count_tbl , tables_dir)


rule aggregate_counts:
    input: 
        sample_counts =  quant_dir + "/{sample}/abundance.tsv", 
        sample_summary = quant_dir + "/{sample}/run_info.json", 
        total_counts_tbl = config["output_dir"] +"/tables/counts_raw.csv",
        total_counts_summary = config["output_dir"] +"/tables/counts_summary.csv"
    output:
        move_done = config["output_dir"] +"/done/counts_moved_{sample}.done"
    params:
        sample_name = "{sample}"
    run: 
        
        kal.aggregate_counts(
            params.sample_name, input.sample_counts, input.total_counts_tbl)
        
        kal.aggregate_summary(
            params.sample_name, input.sample_summary, input.total_counts_summary)
        
        
        shell(
            "touch {output.move_done}"
        )



rule aggregation_complete:
    input: 
        move_done = expand(
            "{out}/done/counts_moved_{sample_id}.done",
            out = config["output_dir"], sample_id = SAMPLES)
    output:
        all_counts_moved = config["output_dir"] + "/done/all_counts_moved.done"
    
    run:
        shell("touch {output.all_counts_moved}")
    



