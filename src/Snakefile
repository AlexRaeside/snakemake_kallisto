
# read in yaml


configfile: "src/test.yaml"


##--- required modules ---------------------------------------------------------

import snakemake_modules.kallisto as kal
import snakemake_modules.metadata as meta
import pandas as pd


##--- global vars --------------------------------------------------------------

fastqc_dir = config["output_dir"] + "/fastqc"
metadata_dir = config["output_dir"] + "/metadata" 
quant_dir = config["output_dir"] + "/quant"
tables_dir = config["output_dir"] + "/tables"

##---- snakemake functions -----------------------------------------------------
# Different then the functions in the modules as snakemake functions
# take one argument (the wildcard) and use global vars like the config data


def fastq_path(sample_name):
    
    # load in metadata 
    meta = pd.read_csv(config["output_dir"] + "/metadata/metadata.csv") 
    sample_info = meta[meta["sample_name"] == str(sample_name)]
    fastq = sample_info["fastqR1_file"].loc[sample_info.index[0]]
    return(fastq)


def get_mean_len(sample_name):
    
    fastqc_report = fastqc_dir + "/" + str(sample_name) + "_fastqc/fastqc_data.txt"
    
    file = open(fastqc_report)
  
    # read the content of the file opened
    length_line =  file.readlines()[8]
    length = length_line.split()[2]
    return(length)


##--- generate metadata --------------------------------------------------------
# Since the sample names in the metadata is a key input parameter for many
# rules. The metadata table is generated as soon as the snakemake is executed 
# unless there is already a metadata and the config does not require a new one


# if there is either new metadata or there is a metadata but the config
# says 

if not os.path.exists(metadata_dir + "/metadata.csv") or config["generate_meta"] == True:
    
    #print( "Generating metadata")
    
    # if the fastq files are give in config use in meta
    if len(config["fastq_R1"]) != 0:
        fq_paths = config["fastq_R1"]
    # if the fastq files are not given in config get from dir 
    if len(config["fastq_R1"]) == 0:
        fq_paths =  meta.get_fastq_in_dir(config["fastq_dir"])
    # use fastq files to generate a metadata table 
    meta.write_sample_metadata(fq_paths, config["output_dir"])
#else:
    #print( "Using prexisting metadata")


# from the metadata get sample names 
# this varibale is used many rules 

SAMPLES = meta.get_samples(metadata_dir + "/metadata.csv")
first_sample = SAMPLES[0]

#print(first_sample)


##--- Snakemake rules ----------------------------------------------------------




rule kallisto_index:
    input:
        fasta = config["fasta"]
    output:
        index = config["output_dir"] + "/index/" + kal.get_indx_name(config["fasta"])
    shell:
        "kallisto index --index={output.index} {input.fasta}"
    
    
rule fastqc:
    input: 
        fq = fastq_path,
        metadata = metadata_dir + "/metadata.csv"
    output:
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    run:
        if not os.path.exists(fastqc_dir):
            os.mkdir(fastqc_dir)
        shell("fastqc -o {fastqc_dir} --extract {input.fq}")



rule kallisto_quant:
    input:
        fq = fastq_path,
        index = config["output_dir"] + "/index/" + kal.get_indx_name(config["fasta"]),
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    output: 
        count_tbl = quant_dir + "/{sample}/abundance.tsv",
        count_summary = quant_dir + "/{sample}/run_info.json",
        quant_done = config["output_dir"] + "/done/quant_{sample}.txt"
    params: 
        thread = config["threads"],
        mean_len = get_mean_len,
        out_path = quant_dir + "/{sample}"
    run:
        if not os.path.exists(quant_dir):
            os.mkdir(quant_dir)
        # get average read length from fastqc report 
        shell(
            "kallisto quant -i {input.index} -o {params.out_path} --single -l {params.mean_len} -s 20  --single-overhang -t {params.thread} {input.fq}")
        shell(
            "touch {output.quant_done}"
        )


rule create_blank_files:
    input:
        count_tbl = quant_dir + "/" + first_sample + "/abundance.tsv"
    output:
        total_counts_tbl = config["output_dir"] +"/tables/counts_raw.csv",
        total_counts_summary = config["output_dir"] +"/tables/counts_summary.csv"
    run:
        
        if not os.path.exists(tables_dir):
            os.mkdir(tables_dir)
        
        kal.create_blank_tbls(input.count_tbl , tables_dir)


rule aggregate_counts:
    input: 
        sample_counts =  quant_dir + "/{sample}/abundance.tsv", 
        sample_summary = quant_dir + "/{sample}/run_info.json", 
        total_counts_tbl = config["output_dir"] +"/tables/counts_raw.csv",
        total_counts_summary = config["output_dir"] +"/tables/counts_summary.csv"
    output:
        move_done = config["output_dir"] +"/done/counts_moved_{sample}.done"
    params:
        sample_name = "{sample}"
    run: 
        
        kal.aggregate_counts(
            params.sample_name, input.sample_counts, input.total_counts_tbl)
        
        kal.aggregate_summary(
            params.sample_name, input.sample_summary, input.total_counts_summary)
        
        
        shell(
            "touch {output.move_done}"
        )



rule aggregation_complete:
    input: 
        move_done = expand(
            "{out}/done/counts_moved_{sample_id}.done",
            out = config["output_dir"], sample_id = SAMPLES)
    output:
        all_counts_moved = config["output_dir"] + "/done/all_counts_moved.done"
    
    run:
        shell("touch {output.all_counts_moved}")
    



