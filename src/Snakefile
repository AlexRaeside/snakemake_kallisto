


##--- required modules ---------------------------------------------------------

import snakemake_modules.kallisto as kal
import snakemake_modules.metadata as meta
import pandas as pd
import os

##--- common vars --------------------------------------------------------------


# conda yaml files placed in the same directory as the snakefile
# used to create virutal enviroments to run shell and R parts 
# of the pipeline

env_kallisto_fastqc = "kallisto_fastqc_conda.yml"
env_rscripts = "rscipt_conda.yml"





# Some variables are used repeatly in the snakemake
# so are stated at the beginning of the pipeline
# this are mostly path strings of the different directories
# snakemake will be writing the output too


fastqc_dir = config["output_dir"] + "/fastqc"
sample_dir = config["output_dir"] + "/sample_table"
quant_dir = config["output_dir"] + "/quant"
tables_dir = config["output_dir"] + "/tables"
metadata_dir = config["output_dir"] + "/metadata"
log_dir = config["output_dir"] + "/log"
figures_dir = config["output_dir"] + "/figures"
done_dir = config["output_dir"] + "/done"
plotly_dir = config["output_dir"] + "/plotly_wedgits"

if not os.path.exists(fastqc_dir):
            os.mkdir(fastqc_dir)

if not os.path.exists(quant_dir):
            os.mkdir(quant_dir)

if not os.path.exists(log_dir):
            os.mkdir(log_dir)

if not os.path.exists(figures_dir):
            os.mkdir(figures_dir)

if not os.path.exists(plotly_dir):
            os.mkdir(plotly_dir)



##---- snakemake functions -----------------------------------------------------
# Different then the functions in the modules as snakemake functions
# take one argument (the wildcard) and use global vars like the config data


def fastq_path(sample_name):

    """
    Using the sample name, return the file path to the corresponding
    fastq file in the sample_table/sample_table.csv. Uses the sample_dir global
    variable to find the sample table.


    """


    # load in sample table
    sample = pd.read_csv(sample_dir + "/sample_table.csv")
    sample_info = sample[sample["sample_name"] == str(sample_name)]
    fastq = sample_info["fastqR1_file"].loc[sample_info.index[0]]
    return(fastq)


def get_mean_len(sample_name):

    """
    Using the sample name, return the mean read length for the sample. The
    mean read length is located in the sample fastqc report which must be produced
    prior to running this function. The function uses the global variable
    fastqc_dir to locate the sample fastqc reports. Returns the read length
    number as a string.

    """


    fastqc_report = fastqc_dir + "/" + str(sample_name) + "_fastqc/fastqc_data.txt"

    file = open(fastqc_report)

    # read the content of the file opened
    length_line =  file.readlines()[8]
    length = length_line.split()[2]
    return(length)


##--- generate sample table ----------------------------------------------------

# Since the sample names in the sample_table is a key input parameter for many
# rules. The sample table is generated as soon as the snakemake is executed
# unless there is already a sample table and the config
# does not state to make a new one each time the snakemake is executed


if not os.path.exists(
    sample_dir + "/sample_table.csv") or config["make_sample_table"] == True:

    """

    By setting config["make_sample_table"] to false the user can add their #
    own sample_table.csv in the sample_table folder. If the user does not state
    to use a preexisting the sample_table.csv then a new sample_table.csv is
    generated either using the list of the fastq file paths in
    config["fastq_R1"] or by selecting all fastq files in the directory in
    config["fastq_dir"]

    """


    # if the fastq files are give in config use in meta
    if len(config["fastq_R1"]) != 0:
        fq_paths = config["fastq_R1"]
    # if the fastq files are not given in config get from dir
    if len(config["fastq_R1"]) == 0:
        fq_paths =  meta.get_fastq_in_dir(config["fastq_dir"])
    # use fastq files to generate a metadata table
    meta.write_sample_table(fq_paths, config["output_dir"])



# From the sample table read the list of samples of process within the
# snakemake. The snakemake will act on process different samples in
# parallel though the first sample will be processed with kallisto_quant
# first in order the get the list and order of targets produced
# to make blank tables where results from all samples can be aggregated

SAMPLES = meta.get_samples(sample_dir + "/sample_table.csv")
first_sample = SAMPLES[0]



##--- Snakemake rules ----------------------------------------------------------



rule move_metadata:
    """
    Copy the metadata file from the path in the config
    to /metadata folder. Filtering to just the cols in the

    """
    input:
        metadata = config["metadata"]
    output:
        metadata = metadata_dir + "/metadata.csv"
    run:
        meta_tbl = meta.move_metadata(
            input.metadata, config["metadata_keep_cols"])
        meta_tbl.to_csv(output.metadata, index = False)





rule kallisto_index:
    """
    Takes the path to [transcripts].fa and creates a kallisto index
    called [transcripts].idx which is used in pseduoalignments. Will
    be run once per snakemake pipeline.

    """
    input:
        fasta = config["fasta"]
    output:
        index = config["output_dir"] + "/index/" + kal.get_indx_name(
            config["fasta"])
    conda: env_kallisto_fastqc
    log: log_dir + "/kallisto_index.log"
    shell:
        "kallisto index --index={output.index} {input.fasta} > {log}"


rule fastqc:
    """

    When running kallisto on single-end reads, an estimate of the average
    read length is needed for kallisto quant. Mean read length is produced along
    with other QC metrics by FastQC.

    This rule uses the 'sample' wildcard which will correspond to any of
    the different samples in the SAMPLES variable.


    """
    input:
        fq = fastq_path,
        sample_tbl = sample_dir + "/sample_table.csv"
    output:
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    params:
        outpath = fastqc_dir
    conda: env_kallisto_fastqc
    log: log_dir + "/fastq_{sample}.log"
    shell: "fastqc -o {params.outpath} --extract {input.fq} > {log}"



rule kallisto_quant:
    """

    Run kallisto quant on samples, using the corresponding fastq file
    from the sample_table.csv, the kallisto index and mean read length
    found in the fastQC report.

    This rule uses the 'sample' wildcard which will correspond to any of
    the different samples in the SAMPLES variable.

    Though it is the lighest RNA-Seq processing program to run there is still
    lots if intereasting stuff going on.


    """
    input:
        fq = fastq_path,
        index = config["output_dir"] + "/index/" + kal.get_indx_name(
            config["fasta"]),
        fastqc_report = fastqc_dir + "/{sample}_fastqc/fastqc_data.txt"
    output:
        count_tbl = quant_dir + "/{sample}/abundance.tsv",
        count_summary = quant_dir + "/{sample}/run_info.json",
        quant_done = config["output_dir"] + "/done/quant_{sample}.txt"
    params:
        thread = config["threads"],
        mean_len = get_mean_len,
        out_path = quant_dir + "/{sample}"
    log: log_dir + "/quant_{sample}.log"
    conda: env_kallisto_fastqc
    shell:
        """
        kallisto quant -i {input.index} -o {params.out_path} \
         --single -l {params.mean_len} -s 20  --single-overhang \
        -t {params.thread} {input.fq} > {log}
        
        touch {output.quant_done}
        """


rule create_tmp_and_summary_files:
    """
    Create two csv files. tmp_counts_raw.csv has a single row target ids
    (gene names) with every fastq sample analysed by kallisto adding a new line
    of target id counts. The tmp_counts_raw.csv has samples as rows and
    target ids as columns, whereas the counts_raw.csv will be transposed
    with samples as cols and targets as rows.
    The counts_summary.csv is a csv file with headers for each of the
    key pseduoalignment stats found in
    """
    input:
        count_tbl = quant_dir + "/" + first_sample + "/abundance.tsv"
    output:
        total_counts_tbl = tables_dir +"/tmp_counts_raw.csv",
        total_counts_summary = tables_dir +"/counts_summary.csv"
    run:
        if not os.path.exists(tables_dir):
            os.mkdir(tables_dir)
        kal.create_blank_tbls(input.count_tbl , tables_dir)


rule aggregate_tmp_counts:
    """
    This rule goes to each of kallisto pseduoalignment counts files produced
    for the different samples and combines the counts into a single
    tables/tmp_counts_summary.csv file.

    It also combines the pseduoalignment stats into a single


    This rule contains the directory resources in order
    to prevent multithreading at this point as that would lead
    to multiple


    """
    input:
        sample_counts =  quant_dir + "/{sample}/abundance.tsv",
        sample_summary = quant_dir + "/{sample}/run_info.json",
        total_counts_tbl = config["output_dir"] +"/tables/tmp_counts_raw.csv",
        total_counts_summary = config["output_dir"] +"/tables/counts_summary.csv"
    output:
        move_done = config["output_dir"] +"/done/counts_moved_{sample}.done"
    resources: threads=1
    params:
        sample_name = "{sample}"
    run:
        kal.aggregate_counts(
            params.sample_name, input.sample_counts, input.total_counts_tbl)
        kal.aggregate_summary(
            params.sample_name, input.sample_summary, input.total_counts_summary)
        shell(
            "touch {output.move_done}"
        )



rule transpose_and_filter_counts:
    """
    Once all counts have been removed. The tmp counts table is loaded, transposed and
    written as counts_raw.csv. As the counts raw is created in this rule it is used as
    input for other rules.

    """
    input:
        move_done = expand(
            "{out}/done/counts_moved_{sample_id}.done",
            out = config["output_dir"], sample_id = SAMPLES),
        tmp_counts = tables_dir + "/tmp_counts_raw.csv"
    output:
        filter_done = config["output_dir"] +"/done/counts_filtered.done"

    params:
        tables_dir = tables_dir
    run:
        kal.transpose_and_filter(
            SAMPLES, tables_dir, config["threads"])

        shell(
            "touch {output.filter_done}"
        )


rule aggregate_counts:
    """

    """
    input:
        filter_done = config["output_dir"] +"/done/counts_filtered.done"
    output:
        filter_aggregate_done =  config["output_dir"] + "/done/filter_aggregate.done",
        filter200 = tables_dir + "/counts_filtered_medi200.csv",
        filter50 = tables_dir +"/counts_filtered_medi50.csv",
        filter25 = tables_dir + "/counts_filtered_medi25.csv",
        filter100 = tables_dir + "/counts_filtered_medi100.csv",
        raw_tbl = tables_dir + "/counts_raw.csv"
    params:
        tables_dir = tables_dir
    shell:
        """
        cat {params.tables_dir}/tmp_0/*.csv >> {params.tables_dir}/counts_raw.csv
        cat {params.tables_dir}/tmp_25/*.csv >> {params.tables_dir}/counts_filtered_medi25.csv
        cat {params.tables_dir}/tmp_50/*.csv >> {params.tables_dir}/counts_filtered_medi50.csv
        cat {params.tables_dir}/tmp_100/*.csv >> {params.tables_dir}/counts_filtered_medi100.csv
        cat {params.tables_dir}/tmp_200/*.csv >> {params.tables_dir}/counts_filtered_medi200.csv

        touch {output.filter_aggregate_done}


        """


rule CPM_normalize:
    """
    This is the first rule which invokes a Rscipt. This is made much easier
    by the fact that the snakemake Script directory is run relative the
    snakefile not the users current working directory.
    
    Rule not run in final version as Rsciprt needs some bug fixes and the CPM
    is only relevent if this was amplicon based seqeucing data not 
    regualr rna-seq. Intreasting to compare to VST but not needed.
    """
    input:
        filter_aggregate_done = config["output_dir"] +"/done/filter_aggregate.done",
        filtered_counts_file = expand(
            "{tables}/counts_filtered_medi{ft}.csv",
            tables = tables_dir,
            ft = ["25", "50", "100", "200"])
    output:
        cpm_counts_file = tables_dir + "/cpm_counts_filtered_medi{ft}.csv"



rule VST_normalize:
    """
    Same as the CPM rule but uses VST normalization. More appropirate to 
    use VST in a PCA analysis. 

    """
    input:
        filter_aggregate_done = config["output_dir"] +"/done/filter_aggregate.done",
        filtered_counts_file = expand(
            "{tables}/counts_filtered_medi{th}.csv",
            tables = tables_dir,
            th = ["25", "50", "100", "200"])
    output:
        vst_counts_file = tables_dir + "/vst_counts_filtered_medi{th}.csv"





rule alignment_summary_figures:
    """
    Produces a figure and plotly wedgit with pseduoalignment 
    summary stats
    """
    input:
        stats_moved = expand(
            "{out}/done/counts_moved_{sample_id}.done",
            out = config["output_dir"], sample_id = SAMPLES),
        stats_summary_file = tables_dir + "/counts_summary.csv"
    output:
        alignment_png = figures_dir + "/pseudoalignment_summary.svg",
        alignment_svg = figures_dir + "/pseduoalignment_summary.png"
    conda: env_rscripts
    params: out = config["output_dir"]
    script: "Rscripts/pseduoalignment_figures.R --out {params.out}"



rule VST_PCA:
    """
    After the counts files are filtered create a PCA.
    For now just run this on counts_filtered_medi100.csv

    """
    input:
        vst_counts_file = tables_dir + "/vst_counts_filtered_medi200.csv",
        metadata_tbl = metadata_dir + "/metadata.csv"
    output:
        vst_pca_png = figures_dir + "/pca_vst_counts_filtered_medi200.png"
    conda: env_rscripts
    params: 
        out = config["output_dir"],
        counts = tables_dir + "/vst_counts_filtered_medi200.csv",
        meta = metadata_dir + "/metadata.csv",
        sample_col = config["metadata_sample_col"],
        exp_col = config["metadata_experiment_col"]
    script: "Rscripts/pca_analysis.R -c {params.counts} -o {params.out} -m {params.meta} -s {params.sample_col} -e {exp_col}"


rule figures_made:
    """"
    Remove all tmp_ files and directories
    Place following firectories in a new directory [run_name]_results/
    
    write config into [run_name]_results/

    """
    input:
        alignment_png = figures_dir + "/pseudoalignment_summary.svg",
        vst_pca_png = figures_dir + "/pca_vst_counts_filtered_medi200.png"
    output:
        tidy_done = done_dir + "/figures_made.done"
